{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7ad0ae",
   "metadata": {},
   "source": [
    "# CIFAR-100 Classification Using Fine-Tuned ResNet-50\n",
    "This notebook uses a pretrained **ResNet-50** model and applies **transfer learning** to classify images in the CIFAR-100 dataset. We use data augmentation, layer freezing, learning rate scheduling, and visualize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchmetrics lightning-utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb7be7",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249166b",
   "metadata": {},
   "source": [
    "## Load CIFAR-100 Dataset with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "classes = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022a0c",
   "metadata": {},
   "source": [
    "## Load and Fine-Tune Pretrained ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(model.fc.in_features, 100)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793999c",
   "metadata": {},
   "source": [
    "## Define Loss, Optimizer, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce35732",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a209f0b",
   "metadata": {},
   "source": [
    "## Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    acc = Accuracy(task='multiclass', num_classes=100).to(device)\n",
    "    total_loss = 0\n",
    "    for X, Y in train_loader:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc.update(preds.argmax(dim=1), Y)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(train_loader.dataset), acc.compute().item()\n",
    "\n",
    "def validate(loader):\n",
    "    model.eval()\n",
    "    acc = Accuracy(task='multiclass', num_classes=100).to(device)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            preds = model(X)\n",
    "            loss = criterion(preds, Y)\n",
    "            acc.update(preds.argmax(dim=1), Y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(loader.dataset), acc.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf499950",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f32bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame()\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch()\n",
    "    val_loss, val_acc = validate(val_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    history = pd.concat([history, pd.DataFrame({\n",
    "        'epoch': [epoch],\n",
    "        'train_loss': [train_loss],\n",
    "        'train_acc': [train_acc],\n",
    "        'val_loss': [val_loss],\n",
    "        'val_acc': [val_acc]\n",
    "    })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7bc84",
   "metadata": {},
   "source": [
    "## Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history['epoch'], history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['epoch'], history['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7ee43",
   "metadata": {},
   "source": [
    "## Final Evaluation and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_acc = Accuracy(task='multiclass', num_classes=100)\n",
    "conf_matrix = ConfusionMatrix(task='multiclass', num_classes=100)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, Y in test_loader:\n",
    "        preds = model(X.to(device)).argmax(dim=1)\n",
    "        test_acc.update(preds.cpu(), Y)\n",
    "        conf_matrix.update(preds.cpu(), Y)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc.compute().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(conf_matrix.compute().numpy(), cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249a811",
   "metadata": {},
   "source": [
    "## Sample Predictions for Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14925749",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(12,6))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(8):\n",
    "        img, label = test_dataset[i]\n",
    "        pred = model(img.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
    "        ax = axes[i//4, i%4]\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "        ax.set_title(f\"True: {classes[label]}\\nPred: {classes[pred]}\")\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
