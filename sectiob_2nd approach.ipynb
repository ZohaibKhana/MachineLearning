{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q8gR17BNn0lR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Section B.‚öôÔ∏èùíüùí∂ùìâùí∂ ùêºùìÉùìâùëíùëîùìáùí∂ùìâùíæùëúùìÉ ùí´ùíæùìÖùëíùìÅùíæùìÉùëí\n"
      ],
      "metadata": {
        "id": "X5GJsfGreFOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Kafka & Zookeeper"
      ],
      "metadata": {
        "id": "f6CLeKR8tA4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Download Kafka** (check https://downloads.apache.org/kafka for stable version)"
      ],
      "metadata": {
        "id": "lf1ab4FGxYH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://downloads.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz\n",
        "!tar -xzf kafka_2.12-3.9.0.tgz\n",
        "!sudo mv kafka_2.12-3.9.0 /usr/local/kafka\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvmv7x2MeEiN",
        "outputId": "f189bf4e-55a5-4125-8987-9aff36956b7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-30 19:07:27--  https://downloads.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122204110 (117M) [application/x-gzip]\n",
            "Saving to: ‚Äòkafka_2.12-3.9.0.tgz‚Äô\n",
            "\n",
            "kafka_2.12-3.9.0.tg 100%[===================>] 116.54M  29.9MB/s    in 4.6s    \n",
            "\n",
            "2025-04-30 19:07:32 (25.1 MB/s) - ‚Äòkafka_2.12-3.9.0.tgz‚Äô saved [122204110/122204110]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Set Kafka environment variables**:\n",
        "\n",
        "First, add environment variables to your .bashrc file, then export them to the current session:"
      ],
      "metadata": {
        "id": "SHQ4-JYMxU8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUkFpWyPd5wR",
        "outputId": "9d0c0cc3-545d-4ed7-b35a-b486e364883a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables set for this session.\n"
          ]
        }
      ],
      "source": [
        "#Set Environment Variables\n",
        "!echo \"export KAFKA_HOME=/usr/local/kafka\" >> ~/.bashrc\n",
        "!echo \"export PATH=\\$PATH:\\$KAFKA_HOME/bin\" >> ~/.bashrc\n",
        "\n",
        "# Export for current session\n",
        "import os\n",
        "os.environ[\"KAFKA_HOME\"] = \"/usr/local/kafka\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"KAFKA_HOME\"] + \"/bin\"\n",
        "\n",
        "print(\"Environment variables set for this session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Install Zookeeper**"
      ],
      "metadata": {
        "id": "NLciNO7SxzKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Zookeeper and kafkacat\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y zookeeper\n",
        "!sudo apt-get install -y kafkacat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czPRwN20d86V",
        "outputId": "e865ff15-ae6e-4959-b8f7-7e75e223560d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 2,588 B/129\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,869 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Fetched 20.8 MB in 4s (5,111 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libjline-java liblog4j1.2-java libslf4j-java libxerces2-java\n",
            "  libxml-commons-external-java libxml-commons-resolver1.1-java\n",
            "  libzookeeper-java\n",
            "Suggested packages:\n",
            "  libjline-java-doc liblog4j1.2-java-doc libmail-java libcommons-logging-java\n",
            "  libxerces2-java-doc libxml-commons-resolver1.1-java-doc\n",
            "  libzookeeper-java-doc\n",
            "The following NEW packages will be installed:\n",
            "  libjline-java liblog4j1.2-java libslf4j-java libxerces2-java\n",
            "  libxml-commons-external-java libxml-commons-resolver1.1-java\n",
            "  libzookeeper-java zookeeper\n",
            "0 upgraded, 8 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 3,911 kB of archives.\n",
            "After this operation, 4,793 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjline-java all 1.0-3 [71.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblog4j1.2-java all 1.2.17-11 [439 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libslf4j-java all 1.7.32-1 [141 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxml-commons-external-java all 1.4.01-5 [240 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxml-commons-resolver1.1-java all 1.2-11 [97.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxerces2-java all 2.12.1-1 [1,437 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libzookeeper-java all 3.4.13-6ubuntu4.1 [1,372 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 zookeeper all 3.4.13-6ubuntu4.1 [112 kB]\n",
            "Fetched 3,911 kB in 1s (3,646 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libjline-java.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libjline-java_1.0-3_all.deb ...\n",
            "Unpacking libjline-java (1.0-3) ...\n",
            "Selecting previously unselected package liblog4j1.2-java.\n",
            "Preparing to unpack .../1-liblog4j1.2-java_1.2.17-11_all.deb ...\n",
            "Unpacking liblog4j1.2-java (1.2.17-11) ...\n",
            "Selecting previously unselected package libslf4j-java.\n",
            "Preparing to unpack .../2-libslf4j-java_1.7.32-1_all.deb ...\n",
            "Unpacking libslf4j-java (1.7.32-1) ...\n",
            "Selecting previously unselected package libxml-commons-external-java.\n",
            "Preparing to unpack .../3-libxml-commons-external-java_1.4.01-5_all.deb ...\n",
            "Unpacking libxml-commons-external-java (1.4.01-5) ...\n",
            "Selecting previously unselected package libxml-commons-resolver1.1-java.\n",
            "Preparing to unpack .../4-libxml-commons-resolver1.1-java_1.2-11_all.deb ...\n",
            "Unpacking libxml-commons-resolver1.1-java (1.2-11) ...\n",
            "Selecting previously unselected package libxerces2-java.\n",
            "Preparing to unpack .../5-libxerces2-java_2.12.1-1_all.deb ...\n",
            "Unpacking libxerces2-java (2.12.1-1) ...\n",
            "Selecting previously unselected package libzookeeper-java.\n",
            "Preparing to unpack .../6-libzookeeper-java_3.4.13-6ubuntu4.1_all.deb ...\n",
            "Unpacking libzookeeper-java (3.4.13-6ubuntu4.1) ...\n",
            "Selecting previously unselected package zookeeper.\n",
            "Preparing to unpack .../7-zookeeper_3.4.13-6ubuntu4.1_all.deb ...\n",
            "Unpacking zookeeper (3.4.13-6ubuntu4.1) ...\n",
            "Setting up libslf4j-java (1.7.32-1) ...\n",
            "Setting up liblog4j1.2-java (1.2.17-11) ...\n",
            "Setting up libjline-java (1.0-3) ...\n",
            "Setting up libxml-commons-resolver1.1-java (1.2-11) ...\n",
            "Setting up libxml-commons-external-java (1.4.01-5) ...\n",
            "Setting up libxerces2-java (2.12.1-1) ...\n",
            "Setting up libzookeeper-java (3.4.13-6ubuntu4.1) ...\n",
            "Setting up zookeeper (3.4.13-6ubuntu4.1) ...\n",
            "adduser: Warning: The home directory `/var/lib/zookeeper' does not belong to the user you are currently creating.\n",
            "update-alternatives: using /etc/zookeeper/conf_example to provide /etc/zookeeper/conf (zookeeper-conf) in auto mode\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  librdkafka1 libyajl2\n",
            "The following NEW packages will be installed:\n",
            "  kafkacat librdkafka1 libyajl2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 683 kB of archives.\n",
            "After this operation, 2,009 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librdkafka1 amd64 1.8.0-1build1 [633 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libyajl2 amd64 2.1.0-3ubuntu0.22.04.1 [21.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 kafkacat amd64 1.6.0-1 [29.0 kB]\n",
            "Fetched 683 kB in 1s (811 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package librdkafka1:amd64.\n",
            "(Reading database ... 126352 files and directories currently installed.)\n",
            "Preparing to unpack .../librdkafka1_1.8.0-1build1_amd64.deb ...\n",
            "Unpacking librdkafka1:amd64 (1.8.0-1build1) ...\n",
            "Selecting previously unselected package libyajl2:amd64.\n",
            "Preparing to unpack .../libyajl2_2.1.0-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package kafkacat.\n",
            "Preparing to unpack .../kafkacat_1.6.0-1_amd64.deb ...\n",
            "Unpacking kafkacat (1.6.0-1) ...\n",
            "Setting up libyajl2:amd64 (2.1.0-3ubuntu0.22.04.1) ...\n",
            "Setting up librdkafka1:amd64 (1.8.0-1build1) ...\n",
            "Setting up kafkacat (1.6.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Set Zookeeper environment variables**:"
      ],
      "metadata": {
        "id": "03BxvP_Lx6v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Zookeeper Environment Variables\n",
        "!echo \"export ZOOKEEPER_HOME=/usr/share/zookeeper\" >> ~/.bashrc\n",
        "!echo \"export PATH=\\$PATH:\\$ZOOKEEPER_HOME/bin\" >> ~/.bashrc\n",
        "\n",
        "# Export for current session\n",
        "import os\n",
        "os.environ[\"ZOOKEEPER_HOME\"] = \"/usr/share/zookeeper\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"ZOOKEEPER_HOME\"] + \"/bin\"\n",
        "\n",
        "print(\"Zookeeper environment variables set for this session.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR55_whed88a",
        "outputId": "3967a330-fa77-4961-c5fa-616f2404042b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zookeeper environment variables set for this session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Start Zookeeper and Kafka**:\n",
        "\n",
        "**Note**: Using full paths to avoid command not found errors"
      ],
      "metadata": {
        "id": "57_XJ9gyyGOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Kafka and Zookeeper\n",
        "# Start Zookeeper first\n",
        "!sudo /usr/share/zookeeper/bin/zkServer.sh start\n",
        "\n",
        "# Add a short delay to ensure Zookeeper is fully started\n",
        "!sleep 5\n",
        "\n",
        "# Start Kafka using the full path\n",
        "!/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties\n",
        "\n",
        "# Add a short delay to ensure Kafka is fully started\n",
        "!sleep 5\n",
        "\n",
        "print(\"Zookeeper and Kafka started.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf9rhZ0Ed8-Z",
        "outputId": "0d027c8f-42c3-4892-e553-9fd19cf97dd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZooKeeper JMX enabled by default\n",
            "Using config: /etc/zookeeper/conf/zoo.cfg\n",
            "Starting zookeeper ... STARTED\n",
            "Zookeeper and Kafka started.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Verify processes are running**:\n",
        "\n",
        "Use jps to check for:\n",
        "- Kafka process\n",
        "- QuorumPeerMain (Zookeeper process)"
      ],
      "metadata": {
        "id": "v2DoFdO5yNSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCZlxX04d9AV",
        "outputId": "6081a955-7f2f-4b49-b13c-28d0af8e1dad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2433 Kafka\n",
            "1987 QuorumPeerMain\n",
            "2614 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Kafka Topic:\n",
        "\n",
        "\n",
        "Using full path to the Kafka command to ensure it works:"
      ],
      "metadata": {
        "id": "17paRFWLyZs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your labWeather topic here\n",
        "!/usr/local/kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic UserEvents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whYs9P8jGxj7",
        "outputId": "fa78a1fb-d4ee-4f4d-aebf-c46e0e9ff19e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic UserEvents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that the topic was created"
      ],
      "metadata": {
        "id": "DWVs26j08dVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 \\  --topic UserEvents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AakXZLMBHB3z",
        "outputId": "3620c68f-0288-4549-8c3a-8dd92a81e7ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: UserEvents\tTopicId: DAb6gawCQYe5d_FOvnGIFw\tPartitionCount: 3\tReplicationFactor: 1\tConfigs: \n",
            "\tTopic: UserEvents\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n",
            "\tTopic: UserEvents\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n",
            "\tTopic: UserEvents\tPartition: 2\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how many partitions and which broker is the leader"
      ],
      "metadata": {
        "id": "bwdiYZ7R94Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe your labWeather topic here\n",
        "!!/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 localhost:2181 \\\n",
        "  --topic UserEvents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3s5tDNAHR0d",
        "outputId": "8cd7dce8-664a-4f45-c0dd-651d81892c8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Command must include exactly one action: --list, --describe, --create, --alter or --delete',\n",
              " 'Option                                   Description                            ',\n",
              " '------                                   -----------                            ',\n",
              " '--alter                                  Alter the number of partitions and     ',\n",
              " '                                           replica assignment. Update the       ',\n",
              " '                                           configuration of an existing topic   ',\n",
              " '                                           via --alter is no longer supported   ',\n",
              " '                                           here (the kafka-configs CLI supports ',\n",
              " '                                           altering topic configs with a --     ',\n",
              " '                                           bootstrap-server option).            ',\n",
              " '--at-min-isr-partitions                  if set when describing topics, only    ',\n",
              " '                                           show partitions whose isr count is   ',\n",
              " '                                           equal to the configured minimum.     ',\n",
              " '--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  ',\n",
              " '  connect to>                              to.                                  ',\n",
              " '--command-config <String: command        Property file containing configs to be ',\n",
              " '  config property file>                    passed to Admin Client. This is used ',\n",
              " '                                           only with --bootstrap-server option  ',\n",
              " '                                           for describing and altering broker   ',\n",
              " '                                           configs.                             ',\n",
              " '--config <String: name=value>            A topic configuration override for the ',\n",
              " '                                           topic being created. The following   ',\n",
              " '                                           is a list of valid configurations:   ',\n",
              " '                                         \\tcleanup.policy                        ',\n",
              " '                                         \\tcompression.gzip.level                ',\n",
              " '                                         \\tcompression.lz4.level                 ',\n",
              " '                                         \\tcompression.type                      ',\n",
              " '                                         \\tcompression.zstd.level                ',\n",
              " '                                         \\tdelete.retention.ms                   ',\n",
              " '                                         \\tfile.delete.delay.ms                  ',\n",
              " '                                         \\tflush.messages                        ',\n",
              " '                                         \\tflush.ms                              ',\n",
              " '                                         \\tfollower.replication.throttled.       ',\n",
              " '                                           replicas                             ',\n",
              " '                                         \\tindex.interval.bytes                  ',\n",
              " '                                         \\tleader.replication.throttled.replicas ',\n",
              " '                                         \\tlocal.retention.bytes                 ',\n",
              " '                                         \\tlocal.retention.ms                    ',\n",
              " '                                         \\tmax.compaction.lag.ms                 ',\n",
              " '                                         \\tmax.message.bytes                     ',\n",
              " '                                         \\tmessage.downconversion.enable         ',\n",
              " '                                         \\tmessage.format.version                ',\n",
              " '                                         \\tmessage.timestamp.after.max.ms        ',\n",
              " '                                         \\tmessage.timestamp.before.max.ms       ',\n",
              " '                                         \\tmessage.timestamp.difference.max.ms   ',\n",
              " '                                         \\tmessage.timestamp.type                ',\n",
              " '                                         \\tmin.cleanable.dirty.ratio             ',\n",
              " '                                         \\tmin.compaction.lag.ms                 ',\n",
              " '                                         \\tmin.insync.replicas                   ',\n",
              " '                                         \\tpreallocate                           ',\n",
              " '                                         \\tremote.log.copy.disable               ',\n",
              " '                                         \\tremote.log.delete.on.disable          ',\n",
              " '                                         \\tremote.storage.enable                 ',\n",
              " '                                         \\tretention.bytes                       ',\n",
              " '                                         \\tretention.ms                          ',\n",
              " '                                         \\tsegment.bytes                         ',\n",
              " '                                         \\tsegment.index.bytes                   ',\n",
              " '                                         \\tsegment.jitter.ms                     ',\n",
              " '                                         \\tsegment.ms                            ',\n",
              " '                                         \\tunclean.leader.election.enable        ',\n",
              " '                                         See the Kafka documentation for full   ',\n",
              " '                                           details on the topic configs. It is  ',\n",
              " '                                           supported only in combination with --',\n",
              " '                                           create if --bootstrap-server option  ',\n",
              " '                                           is used (the kafka-configs CLI       ',\n",
              " '                                           supports altering topic configs with ',\n",
              " '                                           a --bootstrap-server option).        ',\n",
              " '--create                                 Create a new topic.                    ',\n",
              " '--delete                                 Delete a topic                         ',\n",
              " '--delete-config <String: name>           A topic configuration override to be   ',\n",
              " '                                           removed for an existing topic (see   ',\n",
              " '                                           the list of configurations under the ',\n",
              " '                                           --config option). Not supported with ',\n",
              " '                                           the --bootstrap-server option.       ',\n",
              " '--describe                               List details for the given topics.     ',\n",
              " '--exclude-internal                       exclude internal topics when running   ',\n",
              " '                                           list or describe command. The        ',\n",
              " '                                           internal topics will be listed by    ',\n",
              " '                                           default                              ',\n",
              " '--help                                   Print usage information.               ',\n",
              " '--if-exists                              if set when altering or deleting or    ',\n",
              " '                                           describing topics, the action will   ',\n",
              " '                                           only execute if the topic exists.    ',\n",
              " '--if-not-exists                          if set when creating topics, the       ',\n",
              " '                                           action will only execute if the      ',\n",
              " '                                           topic does not already exist.        ',\n",
              " '--list                                   List all available topics.             ',\n",
              " '--partition-size-limit-per-response      the maximum partition size to be       ',\n",
              " '  <Integer: maximum number of              included in one                      ',\n",
              " '  partitions in one response.>             DescribeTopicPartitions response.    ',\n",
              " '--partitions <Integer: # of partitions>  The number of partitions for the topic ',\n",
              " '                                           being created or altered (WARNING:   ',\n",
              " '                                           If partitions are increased for a    ',\n",
              " '                                           topic that has a key, the partition  ',\n",
              " '                                           logic or ordering of the messages    ',\n",
              " '                                           will be affected). If not supplied   ',\n",
              " '                                           for create, defaults to the cluster  ',\n",
              " '                                           default.                             ',\n",
              " '--replica-assignment <String:            A list of manual partition-to-broker   ',\n",
              " '  broker_id_for_part1_replica1 :           assignments for the topic being      ',\n",
              " '  broker_id_for_part1_replica2 ,           created or altered.                  ',\n",
              " '  broker_id_for_part2_replica1 :                                                ',\n",
              " '  broker_id_for_part2_replica2 , ...>                                           ',\n",
              " '--replication-factor <Integer:           The replication factor for each        ',\n",
              " '  replication factor>                      partition in the topic being         ',\n",
              " '                                           created. If not supplied, defaults   ',\n",
              " '                                           to the cluster default.              ',\n",
              " '--topic <String: topic>                  The topic to create, alter, describe   ',\n",
              " '                                           or delete. It also accepts a regular ',\n",
              " '                                           expression, except for --create      ',\n",
              " '                                           option. Put topic name in double     ',\n",
              " \"                                           quotes and use the '\\\\' prefix to     \",\n",
              " '                                           escape regular expression symbols; e.',\n",
              " '                                           g. \"test\\\\.topic\".                    ',\n",
              " '--topic-id <String: topic-id>            The topic-id to describe.This is used  ',\n",
              " '                                           only with --bootstrap-server option  ',\n",
              " '                                           for describing topics.               ',\n",
              " '--topics-with-overrides                  if set when describing topics, only    ',\n",
              " '                                           show topics that have overridden     ',\n",
              " '                                           configs                              ',\n",
              " '--unavailable-partitions                 if set when describing topics, only    ',\n",
              " '                                           show partitions whose leader is not  ',\n",
              " '                                           available                            ',\n",
              " '--under-min-isr-partitions               if set when describing topics, only    ',\n",
              " '                                           show partitions whose isr count is   ',\n",
              " '                                           less than the configured minimum.    ',\n",
              " '--under-replicated-partitions            if set when describing topics, only    ',\n",
              " '                                           show under replicated partitions     ',\n",
              " '--version                                Display Kafka version.                 ']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \\\n",
        "  --describe --topic UserEvents\n"
      ],
      "metadata": {
        "id": "P1IWoIVyd9GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d18866-2ba7-49db-fab4-99ab066915ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Topic: UserEvents\\tTopicId: DAb6gawCQYe5d_FOvnGIFw\\tPartitionCount: 3\\tReplicationFactor: 1\\tConfigs: ',\n",
              " '\\tTopic: UserEvents\\tPartition: 0\\tLeader: 0\\tReplicas: 0\\tIsr: 0\\tElr: N/A\\tLastKnownElr: N/A',\n",
              " '\\tTopic: UserEvents\\tPartition: 1\\tLeader: 0\\tReplicas: 0\\tIsr: 0\\tElr: N/A\\tLastKnownElr: N/A',\n",
              " '\\tTopic: UserEvents\\tPartition: 2\\tLeader: 0\\tReplicas: 0\\tIsr: 0\\tElr: N/A\\tLastKnownElr: N/A']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading the json file"
      ],
      "metadata": {
        "id": "8YVBs5vDCjDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPI4PdMoDCIJ",
        "outputId": "d7775314-4022-475b-ca36-09ae54eeecb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "while true\n",
        "do\n",
        "  curl -s \"https://my.api.mockaroo.com/users.json?key=6be234f0\" |\\\n",
        "  kafka-console-producer --broker-list localhost:9092 --topic UserEvents\n",
        "  sleep 0\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKJYR2qpL-wv",
        "outputId": "4464be6c-7503-467f-ceea-f2f615909671"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!kafkacat -C -b localhost:9092 -t UserEvents -o -5 -e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiqxI75CfECb",
        "outputId": "c2a2c23c-f5f5-4451-93e7-3b9d4f8152f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% Reached end of topic UserEvents [0] at offset 0\n",
            "% Reached end of topic UserEvents [1] at offset 0\n",
            "% Reached end of topic UserEvents [2] at offset 0: exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the topic again, checking for changes"
      ],
      "metadata": {
        "id": "paNgghrQfSlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 \\  --topic UserEvents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMHEvT1ufQbh",
        "outputId": "ee0f2bfc-14f2-41a9-c1db-8d090edc52a8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: UserEvents\tTopicId: DAb6gawCQYe5d_FOvnGIFw\tPartitionCount: 3\tReplicationFactor: 1\tConfigs: \n",
            "\tTopic: UserEvents\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n",
            "\tTopic: UserEvents\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n",
            "\tTopic: UserEvents\tPartition: 2\tLeader: 0\tReplicas: 0\tIsr: 0\tElr: N/A\tLastKnownElr: N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consume messages from the beginning"
      ],
      "metadata": {
        "id": "XHwoMe7Wfk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kafkacat -C -b localhost:9092 -t UserEvents -o beginning -e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MH3njMbfhP4",
        "outputId": "13a7f038-274f-443b-e7b8-d70da1aa533a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% Reached end of topic UserEvents [0] at offset 0\n",
            "% Reached end of topic UserEvents [1] at offset 0\n",
            "% Reached end of topic UserEvents [2] at offset 0: exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# while true\n",
        "#   do\n",
        "#   cat MOCK_DATA.json.json | \\\n",
        "#   kafka-console-producer --broker-list localhost:9092 --topic UserEvents\n",
        "#   sleep 30\n",
        "# done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "CeYijP-xLEpa",
        "outputId": "8fa69d60-739f-43f9-cb1a-c412c08fc819"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-14-aa84e4820e03>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-aa84e4820e03>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    while true; do\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUL6x5Y6irwY",
        "outputId": "4ea85478-81bd-4b0c-bad1-238138613765"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/41.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58748 sha256=a54fda482aeb198ef5017aa588878c09a691f2cd031d51e04c3141dd6ec5e235\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/c2/7c/a53325365fba358ffff35af84a2e14cf88c18052f88acfa5f0\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession with Kafka packages\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaSparkStreaming\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read from Kafka\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"UserEvents\") \\\n",
        "    .load()\n",
        "\n",
        "# Process the streaming data\n",
        "query = kafkaDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(once=True) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "hEXzgEOuiNlF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consumer:"
      ],
      "metadata": {
        "id": "XXmUF2yRBbuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêçüî•Pyspark"
      ],
      "metadata": {
        "id": "q8gR17BNn0lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"first_name\", StringType()),\n",
        "    StructField(\"last_name\", StringType()),\n",
        "    StructField(\"email\", StringType()),\n",
        "    StructField(\"gender\", StringType()),\n",
        "    StructField(\"ip_address\", StringType()),\n",
        "    StructField(\"timestamp\", StringType()),\n",
        "    StructField(\"country\", StringType()),\n",
        "    StructField(\"event_type\", StringType())\n",
        "])\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadJSON\").getOrCreate()\n",
        "df = spark.read.json(\"/content/drive/My Drive/Big_Data/MOCK_DATA.json\", schema=schema)\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "mTAvvkkDd9Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count nulls for each column\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "null_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "null_counts.show()"
      ],
      "metadata": {
        "id": "mgPEaIvl5UDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.dropna()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "pgIJQsQTd9Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "print(\"Unique event_type values:\")\n",
        "df.select(\"event_type\").distinct().show(truncate=False)\n",
        "\n",
        "event_counts = df.groupBy(\"event_type\").agg(count(\"*\").alias(\"count\"))\n",
        "event_counts.show()"
      ],
      "metadata": {
        "id": "__Q2LpXvd9Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the data type of each variable\n"
      ],
      "metadata": {
        "id": "Wzu21BElRbzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name, dtype in df.dtypes:\n",
        "    print(f\"Column: {col_name}, Data Type: {dtype}\")"
      ],
      "metadata": {
        "id": "u22BczVBd9Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "changing the datatype of timestamp"
      ],
      "metadata": {
        "id": "ikdUd8abSXv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import to_timestamp, col\n",
        "# df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n"
      ],
      "metadata": {
        "id": "kIes6HWjd9ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name, dtype in df.dtypes:\n",
        "    print(f\"Column: {col_name}, Data Type: {dtype}\")"
      ],
      "metadata": {
        "id": "-eV9YjPVd9bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting the numbe of duplicates"
      ],
      "metadata": {
        "id": "KVADGfT3TsO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicates\n",
        "duplicate_count = df.count() - df.dropDuplicates().count()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "ybOkkGKFd9d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "uzwMwEv-Zx-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import to_date, date_format, col\n",
        "# Extract date and time\n",
        "df = df.withColumn(\"date\", to_date(\"timestamp\")) \\\n",
        "       .withColumn(\"time\", date_format(\"timestamp\", \"HH:mm:ss\"))\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "58twSgYBd9fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "country wise dekh kon zada appear ho rhahai mrket ka pta lge ga\n",
        "\n",
        "konsa event zda common hai overall\n",
        "\n",
        "evetn ko coun try se group by kr or count ko sath aggregate kr\n",
        "\n",
        "male or female ko comparison krke dkha diffrent event type ko use karke\n",
        "\n",
        "ye dekha k males kia krte zda or females zda kia krte\n",
        "\n",
        "The presence of logout events could help analyze session durations and retention.\n",
        "\n",
        "IP addresses could help identify geographic clusters or detect suspicious activity from similar IPs."
      ],
      "metadata": {
        "id": "FRIu05CGfkmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gender distribution"
      ],
      "metadata": {
        "id": "EvujhL0jISYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"gender\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "JRRBd-ckd9jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Country Distribution"
      ],
      "metadata": {
        "id": "gnA9xDtwIVUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"country\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "YE8EMmfud9l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique Users by Country"
      ],
      "metadata": {
        "id": "_mB38QUrKsWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "df.groupBy(\"country\").agg(countDistinct(\"id\").alias(\"unique_users\")).orderBy(\"unique_users\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "2Zfxz47CKuDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, hour, dayofweek, month, year\n",
        "\n",
        "df = df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
        "       .withColumn(\"weekday\", dayofweek(\"timestamp\")) \\\n",
        "       .withColumn(\"month\", month(\"timestamp\")) \\\n",
        "       .withColumn(\"year\", year(\"timestamp\"))"
      ],
      "metadata": {
        "id": "T1XrPozsd9ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "fN8RmILGd9px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Event Volume by Hour"
      ],
      "metadata": {
        "id": "aaWW5JXwJ715"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"hour\").count().orderBy(\"hour\").show()\n"
      ],
      "metadata": {
        "id": "HQP_Slled9r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Event Trends by Weekday"
      ],
      "metadata": {
        "id": "BdR3ufVJKQAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"weekday\").count().orderBy(\"weekday\").show()\n"
      ],
      "metadata": {
        "id": "YWeVMnd5d9tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day\t          Value Returned\n",
        "Sunday\t      1\n",
        "Monday\t      2\n",
        "Tuesday\t      3\n",
        "Wednesday\t    4\n",
        "Thursday\t    5\n",
        "Friday\t      6\n",
        "Saturday\t    7"
      ],
      "metadata": {
        "id": "3NEKvjNIPSH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top Interactions by Country and Event Type"
      ],
      "metadata": {
        "id": "4wGvESFAKT2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"country\", \"event_type\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "XaDlAoiCd9wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender vs Event Type"
      ],
      "metadata": {
        "id": "7O6rw1Y0KerE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"gender\", \"event_type\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "DTiPLpjqKgJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "df = df.withColumn(\"is_weekend\", when(col(\"weekday\").isin(1, 7), 1).otherwise(0))\n",
        "df = df.withColumn(\"is_night_user\", when((col(\"hour\") >= 0) & (col(\"hour\") <= 6), 1).otherwise(0))\n"
      ],
      "metadata": {
        "id": "5F1kNx-AMjGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  1. Logic:\n",
        "  The weekday column contains the day of the week as an integer (typically from dayofweek() ‚Äî 1 for Sunday, 7 for Saturday).\n",
        "\n",
        "  This line creates a new column is_weekend:\n",
        "\n",
        "\n",
        "  *   0 otherwise (i.e., Monday‚ÄìFriday).\n",
        "  *   1 if the day is Sunday (1) or Saturday (7).\n",
        "\n",
        "  urpose:\n",
        "  To flag if the record/event happened on a weekend.\n",
        "\n",
        "  2. Logic:\n",
        "  - The hour column contains the hour of the day extracted from the timestamp (from 0 to 23).\n",
        "\n",
        "- This line creates a new column is_night_user:\n",
        "\n",
        "  - 1 if the activity happened between midnight (0) and 6 AM (inclusive).\n",
        "\n",
        "  - 0 otherwise.\n",
        "\n",
        "Purpose:\n",
        "To identify users active during nighttime hours (which could imply late-night usage habits)."
      ],
      "metadata": {
        "id": "cXx2X1crP7Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "NIvht7MlM3IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count all 1's in the 'is_weekend' column"
      ],
      "metadata": {
        "id": "XRJ5n9bcOScO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count all 1's in the 'is_weekend' column\n",
        "count_ones = df.filter(col(\"is_weekend\") == 1).count()\n",
        "# Print the result\n",
        "print(f\"Number of activities happend in weekend is : {count_ones}\")"
      ],
      "metadata": {
        "id": "WQvVurhJON_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count all 1's in the 'is_night user' column\n",
        "count_1 = df.filter(col(\"is_night_user\") == 1).count()\n",
        "# Print the result\n",
        "print(f\"Number of activities happend in night is : {count_1}\")"
      ],
      "metadata": {
        "id": "zZ0rXtTzRGfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HDFS(Data Lake Storage)"
      ],
      "metadata": {
        "id": "ufNVbWKTWLrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # query = df.writeStream \\\n",
        "#     .format(\"parquet\") \\\n",
        "#     .option(\"path\", \"gs://your-bucket/output/\") \\\n",
        "#     .option(\"checkpointLocation\", \"gs://your-bucket/checkpoint/\") \\\n",
        "#     .start()"
      ],
      "metadata": {
        "id": "Mq4d1i3rRR-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Machine Learning MLib"
      ],
      "metadata": {
        "id": "JO48moW_ReyN"
      }
    }
  ]
}