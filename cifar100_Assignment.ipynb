{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0786584b",
      "metadata": {
        "id": "0786584b"
      },
      "source": [
        "# CIFAR-100 Image Classification using Pretrained ResNet-34\n",
        "This notebook applies transfer learning using a pretrained ResNet-34 model to classify images from the CIFAR-100 dataset. We improve performance using data augmentation, layer freezing, dropout, a learning rate scheduler, and qualitative analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff1b9a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff1b9a7",
        "outputId": "95d3826e-6209-4f3d-a5d3-2a8991d99772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q torchmetrics lightning-utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf95e227",
      "metadata": {
        "id": "bf95e227"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005d29f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "005d29f4",
        "outputId": "16e2413c-c182-45dd-c20f-df8cdf20f8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torchmetrics\n",
        "from torchmetrics import Accuracy, ConfusionMatrix\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Set computation device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e8bc5d",
      "metadata": {
        "id": "60e8bc5d"
      },
      "source": [
        "## Data Preparation and Augmentation\n",
        "Apply transformations to improve generalization, and prepare data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8a58b2",
      "metadata": {
        "id": "9e8a58b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a589c3aa-e688-4713-a942-2b53a977fec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 63.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define transformations for training and test data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),       # Random crop to increase spatial robustness\n",
        "    transforms.RandomHorizontalFlip(),          # Horizontal flip for augmentation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Color jitter for additional variation\n",
        "    transforms.ToTensor()                       # Convert PIL image to tensor\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor()  # No augmentation for test/validation\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split training into train and validation sets\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0d4730",
      "metadata": {
        "id": "ac0d4730"
      },
      "source": [
        "## Model Setup: ResNet-34 with Transfer Learning\n",
        "We load a pretrained ResNet-34, freeze early layers, and replace the classifier head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "238f3492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238f3492",
        "outputId": "7169b504-67e1-46f1-928a-60a101197d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 83.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained ResNet-34 model\n",
        "model = models.resnet34(weights='IMAGENET1K_V1')\n",
        "# model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Freeze early layers to retain pretrained features\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace the final fully connected layer for CIFAR-100\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.4),  # Dropout for regularization\n",
        "    nn.Linear(model.fc.in_features, 100)  # Output layer for 100 classes\n",
        ")\n",
        "\n",
        "# Move model to appropriate device\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6147bc",
      "metadata": {
        "id": "3d6147bc"
      },
      "source": [
        "## Define Loss, Optimizer and Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8747fd9",
      "metadata": {
        "id": "a8747fd9"
      },
      "outputs": [],
      "source": [
        "# Cross entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AdamW optimizer with weight decay for regularization\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adam optimizer with learning rate 0.001\n",
        "\n",
        "# Learning rate scheduler to reduce LR over time\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb11e91",
      "metadata": {
        "id": "5fb11e91"
      },
      "source": [
        "## Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fdc114",
      "metadata": {
        "id": "a7fdc114"
      },
      "outputs": [],
      "source": [
        "# Train the model for one epoch\n",
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    acc = Accuracy(task='multiclass', num_classes=100).to(device)\n",
        "    total_loss = 0\n",
        "    for X, Y in train_loader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc.update(outputs.argmax(1), Y)\n",
        "        total_loss += loss.item() * X.size(0)\n",
        "    return total_loss / len(train_loader.dataset), acc.compute().item()\n",
        "\n",
        "# Evaluate the model\n",
        "def validate_one_epoch(loader):\n",
        "    model.eval()\n",
        "    acc = Accuracy(task='multiclass', num_classes=100).to(device)\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, Y in loader:\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, Y)\n",
        "            acc.update(outputs.argmax(1), Y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "    return total_loss / len(loader.dataset), acc.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf841df3",
      "metadata": {
        "id": "cf841df3"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03988885",
      "metadata": {
        "id": "03988885"
      },
      "outputs": [],
      "source": [
        "history = pd.DataFrame()\n",
        "epochs = 60\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_one_epoch()\n",
        "    val_loss, val_acc = validate_one_epoch(val_loader)\n",
        "    scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "    history = pd.concat([history, pd.DataFrame({\n",
        "        'epoch': [epoch],\n",
        "        'train_loss': [train_loss],\n",
        "        'train_acc': [train_acc],\n",
        "        'val_loss': [val_loss],\n",
        "        'val_acc': [val_acc]\n",
        "    })], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42854fe",
      "metadata": {
        "id": "a42854fe"
      },
      "source": [
        "## Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1cd86d",
      "metadata": {
        "id": "8c1cd86d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history['epoch'], history['train_acc'], label='Train Acc')\n",
        "plt.plot(history['epoch'], history['val_acc'], label='Val Acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bce87c30",
      "metadata": {
        "id": "bce87c30"
      },
      "source": [
        "## Final Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21951a69",
      "metadata": {
        "id": "21951a69"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_acc = Accuracy(task='multiclass', num_classes=100)\n",
        "conf_matrix = ConfusionMatrix(task='multiclass', num_classes=100)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, Y in test_loader:\n",
        "        preds = model(X.to(device)).argmax(1)\n",
        "        test_acc.update(preds.cpu(), Y)\n",
        "        conf_matrix.update(preds.cpu(), Y)\n",
        "\n",
        "print(\"Test Accuracy:\", test_acc.compute().item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}