{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0GPPFR9RHEb"
   },
   "source": [
    "\n",
    "# **Section B.âš™ï¸ð’Ÿð’¶ð“‰ð’¶ ð¼ð“ƒð“‰ð‘’ð‘”ð“‡ð’¶ð“‰ð’¾ð‘œð“ƒ ð’«ð’¾ð“…ð‘’ð“ð’¾ð“ƒð‘’**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e33648",
   "metadata": {},
   "source": [
    "\n",
    "### Improvements: Streaming and Output Handling\n",
    "\n",
    "To simulate a real-time streaming scenario and avoid reading stale or duplicated data, the following changes were made:\n",
    "1. Added directory cleanup before each run.\n",
    "2. Ensured input and output directories are separate.\n",
    "3. Added a simulated streaming data producer using the existing JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64145d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Simulate streaming by copying lines from MOCK_DATA.json into a stream directory\n",
    "input_json_path = Path(\"MOCK_DATA.json\")\n",
    "streaming_input_dir = Path(\"streaming_input\")\n",
    "streaming_input_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Clean previous streaming input\n",
    "shutil.rmtree(streaming_input_dir)\n",
    "streaming_input_dir.mkdir()\n",
    "\n",
    "# Read the JSON data\n",
    "with open(input_json_path, 'r') as file:\n",
    "    records = json.load(file)\n",
    "\n",
    "# Write each record as a separate line-delimited JSON file (1 file per record)\n",
    "for i, record in enumerate(records[:20]):  # simulate 20 records\n",
    "    with open(streaming_input_dir / f\"event_{i}.json\", 'w') as out_file:\n",
    "        json.dump(record, out_file)\n",
    "    time.sleep(1)  # simulate delay between events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, dayofweek, dayofmonth, month, year\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "input_path = \"streaming_input\"\n",
    "output_path = \"streaming_output\"\n",
    "checkpoint_path = \"streaming_checkpoint\"\n",
    "\n",
    "# Clean old output and checkpoint\n",
    "shutil.rmtree(output_path, ignore_errors=True)\n",
    "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "# Load streaming data\n",
    "streaming_df = spark.readStream.schema(df.schema).json(input_path)\n",
    "\n",
    "# Transform timestamp and extract features\n",
    "streaming_df = streaming_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "streaming_df = streaming_df.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "                           .withColumn(\"weekday\", dayofweek(col(\"timestamp\"))) \\\n",
    "                           .withColumn(\"day\", dayofmonth(col(\"timestamp\"))) \\\n",
    "                           .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "                           .withColumn(\"year\", year(col(\"timestamp\")))\n",
    "\n",
    "# Write stream with proper checkpointing\n",
    "query = streaming_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(30)  # run streaming for 30 seconds then stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBsw6Sv_XTal"
   },
   "source": [
    "# ðŸ› ï¸ *Extraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpCZb3fRYYDT"
   },
   "source": [
    "###  ðŸ“¡Kafka Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5NjIdXzRTWz"
   },
   "source": [
    "`Install Kafka & Zookeeper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RD8C49_ZQ5ve",
    "outputId": "d3c2b9e8-7a9a-4d3f-e485-40036da8cdc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "\r",
      "0% [Connecting to security.ubuntu.com (91.189.91.81)] [Connected to cloud.r-pro\r",
      "                                                                               \r",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "\r",
      "                                                                               \r",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "\r",
      "0% [Connecting to security.ubuntu.com (91.189.91.81)] [Connected to cloud.r-pro\r",
      "                                                                               \r",
      "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "--2025-04-30 10:22:28--  https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
      "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 299350810 (285M) [application/x-gzip]\n",
      "Saving to: â€˜spark-3.3.1-bin-hadoop3.tgz.2â€™\n",
      "\n",
      "spark-3.3.1-bin-had 100%[===================>] 285.48M  4.16MB/s    in 2m 19s  \n",
      "\n",
      "2025-04-30 10:24:48 (2.05 MB/s) - â€˜spark-3.3.1-bin-hadoop3.tgz.2â€™ saved [299350810/299350810]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M34ovGjmS83m"
   },
   "source": [
    "`Simulated Streaming Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7VZwQhkRbmN",
    "outputId": "d41658b8-1aa0-4efa-8294-1399de9720c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated streaming data created.\n"
     ]
    }
   ],
   "source": [
    "# Simulate streaming input files\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Make directories\n",
    "input_dir = '/content/input_stream/'\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Load MOCK data\n",
    "import pandas as pd\n",
    "data = pd.read_json('/content/MOCK_DATA.json')\n",
    "\n",
    "# Stream data: one row at a time into new files\n",
    "for idx, row in data.iterrows():\n",
    "    single_record = pd.DataFrame([row])\n",
    "    single_record.to_json(f'/content/input_stream/record_{idx}.json', orient='records', lines=True)\n",
    "    time.sleep(0.5)  # simulate 0.5 seconds between events\n",
    "\n",
    "print(\"Simulated streaming data created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEqK4cDKTtni"
   },
   "source": [
    "`Creating Sparksession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1BOBvErRbjq",
    "outputId": "ea5c1667-42ae-47f6-f61a-bbd3fd64179f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session started.\n"
     ]
    }
   ],
   "source": [
    "# Start SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ColabStreamingMockData\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "print(\"Spark Session started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueAJP_HUUQ9t"
   },
   "source": [
    "`Defining Schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yZf_q4WCRbhV"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, dayofweek, dayofmonth, month, year\n",
    "\n",
    "# Convert timestamp column to actual timestamp type\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Extract time-based features\n",
    "df = df.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"weekday\", dayofweek(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"day\", dayofmonth(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"year\", year(col(\"timestamp\"))) \\\n",
    "       .withColumn(\"is_weekend\", (dayofweek(col(\"timestamp\")) >= 6).cast(\"int\")) \\\n",
    "       .withColumn(\"is_night_user\", ((hour(col(\"timestamp\")) < 6) | (hour(col(\"timestamp\")) > 20)).cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptvop-wCU5bG"
   },
   "source": [
    "`Read Streaming JSON Data into Spark DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUkUdbATRbfM",
    "outputId": "d249829b-e537-4d71-ad8c-15d6f2ac4877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read stream from input directory\n",
    "\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(\"/content/input_stream/\")\n",
    ")\n",
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKPOq-tBYQmd"
   },
   "source": [
    "# ðŸŒ±Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3cWzS3KY19f"
   },
   "source": [
    "### ðŸðŸ”¥ PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj4Co-5Aengh"
   },
   "source": [
    "`Importing important libraries and functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cmR9jhsHRbdL"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum as spark_sum, count, countDistinct,\n",
    "    to_date, date_format, to_timestamp,\n",
    "    hour, dayofweek, month, year, when\n",
    ")\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcwaMePnUZF2"
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"console\")        # output to console, like .show()\n",
    "    .outputMode(\"append\")     # or \"complete\" depending on your use case\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N686D3Texc5"
   },
   "source": [
    "`Dropping Null Values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LblXVpzERbaq"
   },
   "outputs": [],
   "source": [
    "df_stream = df_stream.na.drop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDOsGe7-fWMy"
   },
   "source": [
    "There was not much null values. Only one null value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4dUtf2nfeST"
   },
   "source": [
    "`Converting timestamp and extract features from it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "L-FHMRBURbYH"
   },
   "outputs": [],
   "source": [
    "df_stream = df_stream.withColumn(\n",
    "    \"timestamp\",\n",
    "    to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    ")\n",
    "df_stream = df_stream.withColumn(\"date\", to_date(\"timestamp\")) \\\n",
    "    .withColumn(\"time\", date_format(\"timestamp\", \"HH:mm:ss\")) \\\n",
    "    .withColumn(\"date\", to_date(\"timestamp\")) \\\n",
    "    .withColumn(\"time\", date_format(\"timestamp\", \"HH:mm:ss\")) \\\n",
    "    .withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "    .withColumn(\"weekday\", dayofweek(\"timestamp\")) \\\n",
    "    .withColumn(\"month\", month(\"timestamp\")) \\\n",
    "    .withColumn(\"year\", year(\"timestamp\")) \\\n",
    "    .withColumn(\"is_weekend\", when(col(\"weekday\").isin(1, 7), 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_night_user\", when((col(\"hour\") >= 0) & (col(\"hour\") <= 6), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTCaalt5f8rJ"
   },
   "source": [
    "* The above line fo code generates the new coloumns in table which are date,time, hour, weekday, month, year , is_weekend and is_night_user.\n",
    "* Please remember that the is_weekend the coloumn with binary values for 1 as weekend meaning its saturday and sunday\n",
    "* Is_night_user the coloumn with hours between 0-6 which is the night time.  \n",
    "* weekday is the coloumn with the numbers from 1-7 and the 1 showing Sunday and 7 as saturday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7EUlAWMhEaK"
   },
   "source": [
    "`Event Type Count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-UIzDMHeRalD"
   },
   "outputs": [],
   "source": [
    "event_counts = df_stream.groupBy(\"event_type\").agg(count(\"*\").alias(\"count\"))\n",
    "event_counts_query = event_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srPmwfwdlirQ"
   },
   "source": [
    "`Count per Gender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-IcVloZkRaif"
   },
   "outputs": [],
   "source": [
    "gender_counts = df_stream.groupBy(\"gender\").count()\n",
    "gender_counts_query = gender_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u2ed3fXlnLM"
   },
   "source": [
    "`Count per Country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4G0zpEYWRaf2"
   },
   "outputs": [],
   "source": [
    "country_counts = df_stream.groupBy(\"country\").count()\n",
    "country_counts_query = country_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM593V-qlyUr"
   },
   "source": [
    "`Unique users per country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "auToqe-VRaZS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "unique_users = df_stream.groupBy(\"country\").agg(approx_count_distinct(\"id\").alias(\"unique_users\"))\n",
    "\n",
    "unique_users_query = unique_users.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PbwA1IIl-ON"
   },
   "source": [
    "`Hourly distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PxScCToFRaXN"
   },
   "outputs": [],
   "source": [
    "hourly = df_stream.groupBy(\"hour\").count()\n",
    "hourly_query = hourly.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUHMIaM8mDIM"
   },
   "source": [
    "`Weekday distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "KclH87bnRaUc"
   },
   "outputs": [],
   "source": [
    "weekday = df_stream.groupBy(\"weekday\").count()\n",
    "weekday_query = weekday.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVzeEkouYP_i"
   },
   "source": [
    "`Country vs Event Type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6Dgk4zw9mMQn"
   },
   "outputs": [],
   "source": [
    "country_event = df_stream.groupBy(\"country\", \"event_type\").count()\n",
    "country_event_query = country_event.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrMXTbuHmSki"
   },
   "source": [
    "` Gender vs Event Type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MP5GGNw2mT3_"
   },
   "outputs": [],
   "source": [
    "gender_event = df_stream.groupBy(\"gender\", \"event_type\").count()\n",
    "gender_event_query = gender_event.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz2-d017oL5e"
   },
   "source": [
    "# ðŸššLoad(Data Lake Storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5hv4B_6_pOJS"
   },
   "outputs": [],
   "source": [
    "query = df_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/content/output_stream/\") \\\n",
    "    .option(\"checkpointLocation\", \"/output/_checkpoint/\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvE2XPDKquTM",
    "outputId": "d8318d7a-c0d1-4ded-bb49-12295ab3ceb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+-----------+---------------+---------+--------------------+----------+----+----+----+-------+-----+----+----------+-------------+\n",
      "| id|first_name|     last_name|               email|     gender|     ip_address|timestamp|             country|event_type|date|time|hour|weekday|month|year|is_weekend|is_night_user|\n",
      "+---+----------+--------------+--------------------+-----------+---------------+---------+--------------------+----------+----+----+----+-------+-----+----+----------+-------------+\n",
      "|835|    Corene|  Dmitrichenko|cdmitrichenkon6@c...|     Female|174.241.178.239|     null|Republic of the C...|     share|null|null|null|   null| null|null|         0|            0|\n",
      "|509|    Barbie|    Komorowski|bkomorowskie4@goo...|     Female|   150.57.58.54|     null|  Dominican Republic|  purchase|null|null|null|   null| null|null|         0|            0|\n",
      "| 46|      Tish|      Christou|tchristou19@marke...|     Female| 218.227.108.73|     null|Palestinian Terri...|   comment|null|null|null|   null| null|null|         0|            0|\n",
      "|344|    Beryle|    Winchcombe|bwinchcombe9j@sci...|     Female|223.241.232.246|     null|              France|   comment|null|null|null|   null| null|null|         0|            0|\n",
      "|494|    Ileane|     Wilkerson|iwilkersondp@netw...|     Female|186.137.197.151|     null|         Philippines|    follow|null|null|null|   null| null|null|         0|            0|\n",
      "|969|    Daphna|     Rowcliffe|drowcliffeqw@page...|     Female|137.237.226.200|     null|         Philippines|    signup|null|null|null|   null| null|null|         0|            0|\n",
      "|253|    Maisie|         Mayou|mmayou70@wundergr...| Polygender|   36.229.89.35|     null|Republic of the C...|     login|null|null|null|   null| null|null|         0|            0|\n",
      "|714|Georgianna|   De Bernardi|gdebernardijt@fre...|    Agender|  229.244.14.21|     null|           Indonesia|  purchase|null|null|null|   null| null|null|         0|            0|\n",
      "|239|   Jacklyn|      Sieghart|jsieghart6m@india...|     Female|   91.240.63.22|     null|  Dominican Republic|    signup|null|null|null|   null| null|null|         0|            0|\n",
      "|349|    Allene|      Swayland|aswayland9o@imgur...|     Female|  118.0.198.103|     null|Palestinian Terri...|  purchase|null|null|null|   null| null|null|         0|            0|\n",
      "|455|  Annabela|   Tunnacliffe|atunnacliffecm@bl...|     Female|110.138.218.213|     null|      Czech Republic|     click|null|null|null|   null| null|null|         0|            0|\n",
      "|497|   Pamella|         Doole|pdooleds@national...|     Female|  41.102.252.47|     null|       United States|  purchase|null|null|null|   null| null|null|         0|            0|\n",
      "|522|  Lauraine|        Menier| lmeniereh@salon.com|     Female|  222.1.171.195|     null|French Southern T...|      view|null|null|null|   null| null|null|         0|            0|\n",
      "|744|     Gaven|        Murphy|gmurphykn@statcou...|   Bigender|  205.154.82.23|     null|Palestinian Terri...|     share|null|null|null|   null| null|null|         0|            0|\n",
      "|855|    Wolfie|    Fettiplace|wfettiplacenq@nif...|       Male| 58.139.232.205|     null|Palestinian Terri...|     click|null|null|null|   null| null|null|         0|            0|\n",
      "| 75|    Darell|     Capponeer|dcapponeer22@surv...|     Female|  175.36.201.91|     null|      Czech Republic|    signup|null|null|null|   null| null|null|         0|            0|\n",
      "|401| Quintilla|  Featherstone|qfeatherstoneb4@s...|     Female|  186.89.27.133|     null|             Nigeria|  purchase|null|null|null|   null| null|null|         0|            0|\n",
      "|418|    Adorne|  Lathleiffure|alathleiffurebl@b...|     Female| 179.45.192.200|     null|            Mongolia|      view|null|null|null|   null| null|null|         0|            0|\n",
      "|436|    Irwinn|De la Perrelle|idelaperrellec3@m...|       Male| 100.52.186.114|     null|              Zambia|    logout|null|null|null|   null| null|null|         0|            0|\n",
      "|570| Ezmeralda|       Kindred|ekindredft@drupal...|Genderqueer| 26.154.181.181|     null|      Czech Republic|    follow|null|null|null|   null| null|null|         0|            0|\n",
      "+---+----------+--------------+--------------------+-----------+---------------+---------+--------------------+----------+----+----+----+-------+-----+----+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/content/output_stream/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_IlUV3ATYDQ"
   },
   "source": [
    "# MLIB"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CpCZb3fRYYDT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
